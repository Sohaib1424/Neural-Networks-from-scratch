{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks\n",
    "Artificial neural networks (ANNs) or simulated neural networks (SNNs) are a subset of machine learning, and are at the heart of deep learning algorithms. Their name and structure are inspired by the human brain, mimicking how biological neurons signal to one another. Neural networks reflect the behavior of the human brain, allowing computer programs to recognize patterns and solve common problems in the fields of AI, machine learning, and deep learning.\n",
    "\n",
    "There are many kinds of neural networks, such as _Convolution Neural Networks(CNNs), Recurrent Neural Networks(RNNs)_ and etc. In this notebook, we'll be implementing a simple neural network from scratch just using NumPy. We'll have a brief explanation of the similarity  between _Logistic Regression_ and a simple _Neural Net_, and we'll build up our model from that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning goals\n",
    "\n",
    "- Learning the similarity between _Logistic Regression_ and a simple _Neural Network_\n",
    "- Implementing a simple _neural network_ to recognize handwritten digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- [1] [Stanford CS229 Machine Learning](https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU), [CS229 lecture notes](http://cs229.stanford.edu/syllabus.html)\n",
    "- [2] [Seyed Naser Razavi's machine learning class](https://www.youtube.com/playlist?list=PLW529xl11jnnupZKT5Og4pwHPoRFQRQz_) (Persian)\n",
    "- [3] [The Softmax Function Derivative](https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_test_split(X, y, val_size=.2, test_size=.2, shuffle=False):\n",
    "    if val_size < 0 or test_size < 0:\n",
    "        \n",
    "        raise EnvironmentError(\"validation or test size should not be negative!\")\n",
    "    \n",
    "    if val_size + test_size >= 1.:\n",
    "        raise EnvironmentError(\"validation + test size should be lesser than 1!\")\n",
    "    \n",
    "    x_data, y_data = X, y\n",
    "    if shuffle:\n",
    "        for _ in range(3):\n",
    "            temp = list(zip(x_data.T, y_data.T))\n",
    "            random.shuffle(temp)\n",
    "            x_data, y_data = zip(*temp)\n",
    "            x_data = np.array(list(x_data)).T\n",
    "            y_data = np.array(list(y_data)).T\n",
    "        \n",
    "    \n",
    "    val_indx = int(val_size * x_data.shape[1])\n",
    "    test_indx = int(test_size * x_data.shape[1])\n",
    "    X_val, X_test, X_train = x_data[:, :val_indx], x_data[:, val_indx:val_indx+test_indx], x_data[:, val_indx+test_indx:]\n",
    "    y_val, y_test, y_train = y_data[:, :val_indx], y_data[:, val_indx:val_indx+test_indx], y_data[:, val_indx+test_indx:]\n",
    "    \n",
    "    return X_train, X_test, X_val, y_train, y_test, y_val\n",
    "\n",
    "def accuracy(y_pred, y_true):\n",
    "    return 100. * sum(np.argmax(y_pred, axis=0) == np.argmax(y_true, axis=0)) / y_true.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rectified linear unit\n",
    "def ReLU(x):\n",
    "    x = np.where(x < 0, 0, x)\n",
    "    return x\n",
    "# derivative of ReLU\n",
    "def dReLU(x):\n",
    "    x = np.where(x < 0, 0, x)\n",
    "    x = np.where(x > 0, 1, x)\n",
    "    return x\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def dtanh(x):\n",
    "    return 1. - tanh(x)**2\n",
    "\n",
    "# sigmoid function\n",
    "def sigm(x):\n",
    "    return 1. / (1. + np.exp(-x))\n",
    "\n",
    "# derivative of sigmoid function\n",
    "def dsigm(x):\n",
    "    z = sigm(x)\n",
    "    return z * (1. - z)\n",
    "def softmax(x):\n",
    "    x = x - np.max(x, axis=0, keepdims=True)\n",
    "    exp_x = np.exp(x)\n",
    "    exp_x /= np.sum(exp_x, axis=0, keepdims=True)\n",
    "    return exp_x\n",
    "def dsoftmax(y_hat, y):\n",
    "    return y_hat - y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "_Logistic Regression_ is one of the fundamental and popular algorithms to solve classification problems. As an example, suppose we want to classify images based on whether there is a cat in them or not. So, the presence of a cat is $y = 1$, and the absence of a cat will be $y = 0$.\n",
    "\n",
    "The first thing we need to do to classify cat images using Logistic Regression is to vectorize the images. We know that images can be represented in 3D matrices, so if we take the color cat image below of size $64\\times64\\times3$ (3 for the RGB channel) and flatten it into a vector, we'll have 12288 numbers to represent those pixels. We'll call it $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='imgs/cat0.png' width='70%' height='70%' style=\"float:center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, since we have an input vector $x$, we can take it and push it in the logistic operation (as illustrated below), which has two parts. The first part (the linear part) is $Wx + b$, and the second part (the activation part) is the sigmoid function. The sigmoid function is a function that takes a number between $+\\infty$ and $-\\infty$, then maps it between 1 and 0. The operations' output is a number showing the probability of presence of a cat in the image, and it's called $\\hat{y}$.\n",
    "\n",
    "$$\\hat{y} = \\sigma(Wx + b) = \\frac{1}{1 + e^{-(Wx + b)}} \\tag{1}$$\n",
    "The larger the $\\hat{y}$ is, the higher the probability of a cat being in the input image is.\n",
    "\n",
    "Bear in mind that since the shape of vector input $x$ is (12288, 1), $W$ should be of shape (1, 12288), and $b$ will be just a single number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='imgs/cat1.jpg' width='70%' height='70%' style=\"float:center\">\n",
    "\n",
    "From now on, we'll call the __multi-class logistic regression__ model illustrated above a __neuron__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a neuron, we need to train it (obviously after initializing the weights and the bias). To do that, we need to find the optimal weights and the optimal bias. After finding the optimal values, we'll use them to predict.\n",
    "\n",
    "Finding the optimal $W$ and $b$ means defining a loss function. That loss function would be called the __Logistic loss__, which we want to minimize it for each input we give to it:\n",
    "$$\\begin{align}\n",
    "\\mathscr{L} &= - (ylog\\hat{y} + (1 - y)log(1 - \\hat{y})) \\tag{2}\\\\\n",
    "W &= W - \\alpha \\frac{\\partial\\mathscr{L}}{\\partial W}\\\\\n",
    "b &= b - \\alpha \\frac{\\partial\\mathscr{L}}{\\partial b}\n",
    "\\end{align}$$\n",
    "\n",
    "And, if we give the neuron $m$ inputs at the same time, the _Logistic loss_ becomes:\n",
    "\n",
    "$$\\mathscr{L} = - \\frac1m \\sum_{i=1}^m(y^{(i)}log\\hat{y}^{(i)} + (1 - y^{(i)})log(1 - \\hat{y}^{(i)}))\\tag{3}$$\n",
    "\n",
    "And $X$ is:\n",
    "\n",
    "$$\n",
    "X = \n",
    "\\begin{pmatrix}\n",
    "\\big| & \\big| & \\cdots & \\big|\\\\\n",
    "x^{(1)} & x^{(2)} & \\ddots & x^{(m)}\\\\\n",
    "\\big| & \\big| & \\cdots & \\big|\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "$y^{(i)}$ denotes the correct answer for $x^{(i)}$.\n",
    "\n",
    "Now, what if we want to classify other things, like a dog, a mouse, etc.? To do that, we'll add more neurons to our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='imgs/multiClass.png' width='40%' height='40%' style=\"float:center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, since we have a __Layer__ of neurons, the shape of $W$ becomes $(n, 12288)$ and we also have $n$ biases ($n$ is the number of neurons in the layer, or the number of classes in the model). Also, the correct label of the input image, and the operations' output $\\hat{y}$ become vectors of size $n$:\n",
    "\n",
    "$$\\begin{align}\n",
    "y = \\pmatrix{y_1=0 \\\\ y_2=0 \\\\ .\\\\ y_j=1\\\\. \\\\ .\\\\ y_k=1 \\\\ . \\\\ y_n=0}, \\hat{y} &= \\pmatrix{\\hat{y}_1 \\\\ \\hat{y}_2 \\\\ .\\\\ . \\\\ . \\\\ \\hat{y}_n} \\\\\n",
    "\\hat{y}_i = \\sigma(W_i x + b_i) &= \\frac{1}{1 + e^{-(W_i x + b_i)}} \\forall i \\in {1,...,n}\\tag{4}\n",
    "\\end{align}$$\n",
    "\n",
    "The reason for having $y_j=1$ and $y_k=1$ is that the image might include multiple classes. Like an image of a cat and a dog.\n",
    "\n",
    "And the _Logistic loss_ function will be:\n",
    "$$\\mathscr{L} = \\sum_{i=1}^n -(y_ilog\\hat{y}_i + (1-y_i)log(1-\\hat{y}_i))\\tag{5}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, if $m$ inputs are given to our model, then $y$ and $\\hat{y}$ become matrices of shape (n, m), and the _Logistic loss_ becomes:\n",
    "$$\\mathscr{L} = \\frac1m \\sum_{i=1}^m \\mathscr{L}^{(i)} = -\\frac1m \\sum_{i=1}^m\\sum_{j=1}^n (y_j^{(i)}log\\hat{y}_j^{(i)} + (1-y_j^{(i)})log(1-\\hat{y}_j^{(i)}))\\tag{6}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model might work well in some cases. But, there are cases that we have a __constraint__ where there is only one possible outcome.\n",
    "- Think about healthcare. There are many models made to detect if a specific disease exists based on cell microscopic images. Usually, there are no overlaps among diseases, meaning we want to classify a particular disease among a large number of diseases. The model above would still work, but it will not be optimal because it takes longer to train. Maybe one disease is super rare, and therefore one of the neurons would rarely get trained. Thus we'll work with another model and put on the constraint, and we let the neurons learn by creating interactions between them.\n",
    "\n",
    "\n",
    "- Another example is classifying __HandWritten Digits__ since the given input image belongs to only one possible outcome (not two or more). We'll be focusing on this example and implement a model that can classify handwritten digits.\n",
    "\n",
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
<<<<<<< HEAD
<<<<<<< HEAD
    "data_train = pd.read_csv(\"data\\\\train.csv\")\n",
||||||| 972543e
    "file_path = \"C://Users//cafestock//Desktop//data analysis//ML//Neural Networks//data\"\n",
    "data_train = pd.read_csv(file_path +\"//train.csv\")\n",
=======
    "data_train = pd.read_csv(\"data\\\train.csv\")\n",
>>>>>>> 359f51a08db72fcd07efe57db2d14987bd15710c
||||||| 359f51a
    "data_train = pd.read_csv(\"data\\\train.csv\")\n",
=======
    "data_train = pd.read_csv(\"data//train.csv\")\n",
>>>>>>> 814c11b7b53850508a30c6ff4f603420b81aa07c
    "\n",
    "x = np.array(data_train.drop(columns=[\"label\"])).T /256.\n",
    "y = np.zeros((10, 42000))\n",
    "\n",
    "for i in range(42000):\n",
    "    y[data_train.iloc[i][\"label\"], i] = 1.\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(x[:, random.randint(0, 41999)].reshape(28, 28), interpolation='nearest', cmap=plt.cm.Greys)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title(\"Random sample!\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first, let's deal with the constraint using __softmax__.\n",
    "\n",
    "## Softmax Classifier\n",
    "\n",
    "In the multi-class logistic regression model that we defined above (using sigmoid function), we are training $n$ different classifiers. Therefore, the sum of the probabilities for different classes might not be __1__. In this new model, in which we're training $n = 10$ classifiers, after computing the linear part ($W_ix + b_i$) in each neuron, we will use __softmax__ instead of sigmoid function in the activation part. Thus, instead of having $n$ different classifiers which are acting separately, we have only one classifier which is able to distinguish among $n$ different classes, and the sum of the probabilities equals __1__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='imgs/softmax_classifier_for_digits.png' width='50%' height='50%' style=\"float:center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Function\n",
    "\n",
    "Suppose we have a vector $z$ consisting of the results computed in the linear part and related to different classes for given input $x$ ($z \\in \\mathbb{R}^{10}$). Then, $softmax(z)$ computes a vector of probabilities (one probability for each class). In fact, the $k$th element of this vector represents the probalitity of $x$ being of class $k$. This is computed as following:\n",
    "$$\\begin{align}z &= Wx + b\\\\\n",
    "s_k = \\text{softmax}(z)_k &= \\frac{e^{z_k}}{\\sum_{i=1}^{10} e^{z_i}}, \\forall k \\in 1,...,10 \\tag{7}\\\\\\\\\n",
    "\\hat{y} = s &= \\pmatrix{s_1 \\\\ s_2 \\\\ . \\\\ . \\\\ . \\\\ s_{10}}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Cross Entropy Loss Function\n",
    "\n",
    "Now, we need a new loss function to use for finding the optimal weights and the optimal bias. This loss function is called __Categorical Cross Entropy Loss Function__:\n",
    "\n",
    "$$\\mathscr{L} = -\\sum_{i=1}^{10}y_ilog(s_i)\\tag{8}$$\n",
    "In which $y$ is the correct class for given input $x$. Surprisingly, it's very similar to the _Logistic loss_. The reason for it not having the $(1 - y)log(1 - s)$ part is explained in the following.\n",
    "\n",
    "Suppose that a given input $x$ belongs to class $k$. Thus, all $y_i$s are 0, except for $y_k$ which is 1. So, $\\mathscr{L}(x) = -\\sum_{i=1}^{10}y_ilog(s_i) = -y_klog(s_k) = -log(s_k)$.\n",
    "Minimizing $\\mathscr{L}$ means maximizing $log(s_k)$, or maximizing $s_k$. Maximizing $s_k$ will result in minimizing other $s_i$s consequently, since $\\sum_{i=1}^{10} s_i = 1$ and $0 \\le s_i \\le 1$. That's the reason for not including $(1 - y)log(1 - s)$ in the equation.\n",
    "\n",
    "Just a reminder, with $m$ inputs, $y$ and $s$ become metrices of shape (n, m), or (10, m), and the loss function will be:\n",
    "\n",
    "$$\\mathscr{L} = -\\frac1m \\sum_{i=1}^{m}\\sum_{j=1}^{10}y_j^{(i)}log(s_j^{(i)})\\tag{9}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "Let's cumpute $\\frac{\\partial\\mathscr{L}}{\\partial W}$ and $\\frac{\\partial\\mathscr{L}}{\\partial b}$ to find the optimal values for $W$ and $b$. Using the chain rule we get:\n",
    "$$\\begin{align} \n",
    "\\frac{\\partial\\mathscr{L}}{\\partial W} &= \\sum_{i=1}^{10} \\frac{\\partial\\mathscr{L}}{\\partial s_i} \\cdot \\frac{\\partial s_i}{\\partial z} \\cdot \\frac{\\partial z}{\\partial W} \\tag{10}\\\\\\\\\n",
    "\\frac{\\partial\\mathscr{L}}{\\partial b} &= \\sum_{i=1}^{10} \\frac{\\partial\\mathscr{L}}{\\partial s_i} \\cdot \\frac{\\partial s_i}{\\partial z} \\cdot \\frac{\\partial z}{\\partial b} \\tag{11}\n",
    "\\end{align}$$\n",
    "\n",
    "Solving for $\\frac{\\partial\\mathscr{L}}{\\partial s_i}$:\n",
    "$$\\frac{\\partial\\mathscr{L}}{\\partial s_i} = \\frac{\\partial}{\\partial s_i}(-y_ilog(s_i) + \\sum_{j \\neq i} -y_jlog(s_j)) = -\\frac{y_i}{s_i}\\tag{12}$$\n",
    "\n",
    "Solving for $\\frac{\\partial s}{\\partial z}$: when talking about the derivative of the softmax function, we actually talk about its Jacobian matrix, which is the matrix of all first-order partial derivatives:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial s}{\\partial z} = J_{softmax} = \n",
    "\\pmatrix{\\frac{\\partial s_1}{z} \\\\ \\frac{\\partial s_2}{z} \\\\ \\vdots \\\\ \\frac{\\partial s_i}{z}  \\\\ \\vdots \\\\ \\frac{\\partial s_{10}}{z}} =\n",
    "\\begin{pmatrix}\\frac{\\partial s_1}{\\partial z_1} &\n",
    "\\frac{\\partial s_1}{\\partial z_2} & \\cdots & \\frac{\\partial s_1}{\\partial z_{10}} \\\\\n",
    "\\frac{\\partial s_2}{\\partial z_1} & \\frac{\\partial s_2}{\\partial z_2} & \\cdots &\n",
    "\\frac{\\partial s_2}{\\partial z_{10}}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\frac{\\partial s_{10}}{\\partial z_1} & \\frac{\\partial s_{10}}{\\partial z_2} & \\cdots &\n",
    "\\frac{\\partial s_{10}}{\\partial z_{10}}\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the outputs of the softmax function are strictly positive values, we can make the above derivations super short, by applying the following trick: instead of taking the partial derivatives of the outputs, we take the partial derivatives of the log of the outputs (also called __logarithmic derivative__):\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial}{\\partial z_j}log(s_i) = \\frac{1}{s_i} \\cdot \\frac{\\partial s_i}{\\partial z_j} \\\\\n",
    "\\frac{\\partial s_i}{\\partial z_j} = s_i \\cdot \\frac{\\partial}{\\partial z_j}log(s_i)\\tag{13}\n",
    "\\end{align}$$\n",
    "\n",
    "And $log(s_i)$ is:\n",
    "$$log(s_i) = log(\\frac{e^{z_i}}{\\sum_{l=1}^{10} e^{z_l}}) = z_i - log(\\sum_{l=1}^{10} e^{z_l})$$\n",
    "\n",
    "The partial derivative of the above expression is:\n",
    "$$\\frac{\\partial}{\\partial z_j}log(s_i) = \\frac{\\partial z_i}{\\partial z_j} - \\frac{\\partial}{\\partial z_j}log(\\sum_{l=1}^{10} e^{z_l})\\tag{14}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the first term on the right hand side:\n",
    "$$\n",
    "\\frac{\\partial z_i}{\\partial z_j} = \\bigg\\{ \\begin{array} \\text{1}, & \\text{if} \\ \\ i = j \\\\ 0, & otherwise \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "It can be replaced with the indicator function $1\\{arg\\}$ (the indicator function returns 1 if its argument is true, and 0 otherwise). Let's rewrite Eq.14 :\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial}{\\partial z_j}log(s_i) &= 1\\{i = j\\} - \\frac{1}{\\sum_{l=1}^{10} e^{z_l}} \\cdot \\ (\\frac{\\partial}{\\partial z_j}\\sum_{l=1}^{10} e^{z_l}) \\\\\n",
    "&= 1\\{i = j\\} - \\frac{e^{z_j}}{\\sum_{l=1}^{10} e^{z_l}}\\\\ \n",
    "&= 1\\{i = j\\} - s_j\\tag{15}\n",
    "\\end{align}$$\n",
    "\n",
    "From equations 13 and 15, we have:\n",
    "$$\\frac{\\partial s_i}{\\partial z_j} = s_i \\cdot (1\\{i = j\\} - s_j) \\tag{16}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the jacobian matrix becomes:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial s}{\\partial z} = J_{softmax} =\n",
    "\\begin{pmatrix}\n",
    "s_1 \\cdot (1 - s_1) & -s_1\\cdot s_2 & \\cdots & -s_1\\cdot s_{10}\\\\\n",
    "-s_2 \\cdot s_1 & s_2 \\cdot (1 - s_2) & \\cdots & -s_2\\cdot s_{10}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "-s_{10} \\cdot s_1 & -s_{10} \\cdot s_2 & \\cdots & s_{10}\\cdot (1 - s_{10})\\\\\n",
    "\\end{pmatrix} \\tag{17}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, from the results from equations 12 and 17 and using the __broadcasting technique__, we'll get:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\mathscr{L}}{\\partial s} \\cdot \\frac{\\partial s}{\\partial z} = \n",
    "\\pmatrix{\\frac{y_1}{s_1} \\\\ \\frac{y_2}{s_2}\\\\ \\vdots \\\\ \\frac{y_{10}}{s_{10}}}\n",
    "\\pmatrix\n",
    "{\n",
    "s_1 \\cdot (s_1 - 1) & s_1\\cdot s_2 & \\cdots & s_1\\cdot s_{10}\\\\\n",
    "s_2 \\cdot s_1 & s_2 \\cdot (s_2 - 1) & \\cdots & s_2\\cdot s_{10}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "s_{10} \\cdot s_1 & s_{10} \\cdot s_2 & \\cdots & s_{10}\\cdot (s_{10} - 1)\\\\\n",
    "} =\n",
    "\\pmatrix\n",
    "{\n",
    "0 & 0 & 0 & 0 & \\cdots & 0\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "s_1 & s_2 & \\cdots & s_k - 1 & \\cdots & s_{10}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "0 & 0 & 0 & 0 & \\cdots & 0\\\\\n",
    "}_{10 \\times 10}\\tag{18}\n",
    "$$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$ \n",
    "\\sum_{i=1}^{10} \\frac{\\partial\\mathscr{L}_i}{\\partial s_i} \\cdot \\frac{\\partial s_i}{\\partial z} = \n",
    "\\begin{pmatrix}s_1 \\\\ s_2 \\\\ \\vdots \\\\ s_k - 1 \\\\ \\vdots \\\\ s_{10} \\end{pmatrix} =\n",
    "\\begin{pmatrix}s_1 \\\\ s_2 \\\\ \\vdots \\\\ s_k \\\\ \\vdots \\\\ s_{10} \\end{pmatrix} - \n",
    "\\begin{pmatrix}y_1=0 \\\\ y_2=0 \\\\ \\vdots \\\\ y_k = 1 \\\\ \\vdots \\\\ y_{10}=0 \\end{pmatrix}=\n",
    "s - y\n",
    "\\tag{19}\n",
    "$$\n",
    "\n",
    "Bear in mind that since the input vector $x$ belongs to class $k$, all $y_i$s are 0 except for $y_k$, which is 1. That's why there is only one nonzero row in the $10 \\times 10$ matrix above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_cross_entropy_cost(y_hat, y):\n",
    "    cost = np.mean(-np.sum(y * np.log(y_hat+1e-9), axis=0))\n",
    "    return cost\n",
    "\n",
    "def dsoftmax_cross_entropy_cost(y_hat, y):\n",
    "    # equation 19: derivative of Loss with respect to z\n",
    "    return dsoftmax(y_hat, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we plug Eq19 back into Eq10 and Eq11 to compute $\\frac{\\partial\\mathscr{L}}{\\partial W}$ and $\\frac{\\partial\\mathscr{L}}{\\partial b}$. Two derivations remain, $\\frac{\\partial z}{\\partial W}$ and $\\frac{\\partial z}{\\partial b}$ :\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial z}{\\partial W} &= \\frac{\\partial}{\\partial W}(Wx + b) = x^T \\\\\n",
    "\\frac{\\partial z}{\\partial b} &= \\frac{\\partial}{\\partial b}(Wx + b) = \n",
    "\\pmatrix{1 \\\\ 1 \\\\ \\vdots \\\\ 1}_{10 \\times 1}\n",
    "\\end{align}$$\n",
    "\n",
    "The reason for $\\frac{\\partial z}{\\partial W} = x^T$ is that $\\frac{\\partial z}{\\partial W}$ needs to be of shape (10, 12288), just like $W$, and the only way to achieve this is to take the dot product of vector $s - y$ with shape (10, 1) and vector $x^T$ with shape (1, 12288). \n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial\\mathscr{L}}{\\partial W} &= (s - y)x^T \\\\\\\\\n",
    "\\frac{\\partial\\mathscr{L}}{\\partial b} &= s - y\\\\\\\\\n",
    "W &= W - \\alpha \\frac{\\partial\\mathscr{L}}{\\partial W} \\\\\n",
    "b &= b - \\alpha \\frac{\\partial\\mathscr{L}}{\\partial b}\\\\\n",
    "(\\alpha \\ &\\text{is the learning rate})\n",
    "\\end{align}$$\n",
    "\n",
    "And once more, given $m$ inputs, $\\frac{\\partial\\mathscr{L}}{\\partial z} = \\frac{\\partial\\mathscr{L}}{\\partial s} \\cdot \\frac{\\partial s}{\\partial z} = y - s$ would be of shape $(10, m)$, and every coulumn of it, as you would already know, equals $s^{(i)} - y^{(i)}$ for each given $x^{(i)}$, and $\\frac{\\partial\\mathscr{L}}{\\partial W} = \\frac1m \\cdot (s - y)X^T$ (we divide by $m$ because of Eq9).\n",
    "\n",
    "Let's see what we got so far. We have a model with a layer of ten neurons, we have our data which we're going to train the model with it, and we know how to get it to work. OK! No more explanations, for now. Further down the road, we'll add more layers, and we'll find out how to optimize for that.\n",
    "\n",
    "__Now, Let's see all of this in practice.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, X_val, y_train, y_test, y_val = train_valid_test_split(x, y, test_size=.15, val_size=.15, shuffle=True)\n",
    "# mean value of features\n",
    "mu = np.mean(X_train, axis=1, keepdims=True)\n",
    "\n",
    "# remove mean vector from all data\n",
    "X_train -= mu\n",
    "X_val -= mu\n",
    "X_test  -= mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how `mu` variable looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(mu.reshape(28, 28), interpolation='nearest', cmap=plt.cm.Greys)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title(\"Mean value of features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following class, allows us to create layers with varying neuron counts\n",
    "class Layer:\n",
    "    \n",
    "    def __init__(self, in_shape, n_neurons, Activation=\"ReLU\"):\n",
    "        if in_shape <= 0 or n_neurons <= 0:\n",
    "            raise EnvironmentError(\"Input values should be positive!\")\n",
    "        \n",
    "        self.shape = np.array([n_neurons, in_shape])\n",
    "        self.b = np.zeros((self.shape[0], 1)) # the biases\n",
    "        \n",
    "        # Initializing Wieghts and specifying activation function for the layer\n",
    "        if Activation == \"ReLU\":\n",
    "            self.Activation = ReLU\n",
    "            self.dActivation = dReLU\n",
    "            self.W = np.random.randn(*self.shape) * np.sqrt(2. / self.shape[1])\n",
    "        elif Activation == \"sigm\":\n",
    "            self.Activation = sigm\n",
    "            self.dActivation = dsigm\n",
    "            self.W = np.random.randn(*self.shape) * np.sqrt(1. / self.shape[1])\n",
    "        elif Activation == \"tanh\":\n",
    "            self.Activation = tanh\n",
    "            self.dActivation = dtanh\n",
    "            self.W = np.random.randn(*self.shape) * np.sqrt(1. / sum(self.shape))\n",
    "        elif Activation == \"Linear\":\n",
    "            self.Activation = lambda x: x\n",
    "            self.dActivation = lambda x: 1.\n",
    "            self.W = np.random.randn(*self.shape)\n",
    "        elif Activation == \"soft\":\n",
    "            self.Activation = softmax\n",
    "            self.dActivation = dsoftmax\n",
    "            self.W = np.random.randn(*self.shape) * 0.001\n",
    "        else:\n",
    "            raise EnvironmentError(\"Activation function not defined!\")\n",
    "            \n",
    "        self.x = None # the given input to the Layer        \n",
    "        self.z = None # the result from the linear part (Wx + b)\n",
    "        self.a = None # the result from the activation part (activation(z))\n",
    "        \n",
    "    def train(self, x):\n",
    "        self.x = x\n",
    "        self.z = self.W @ self.x + self.b # computing the linear part\n",
    "        self.a = self.Activation(self.z) # computing the activation part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with the help of the following class we can create a Neural Network model with multiple and different layers,\n",
    "# choose a specific lost function to train the model based on it, define desirable learning rate, set the number\n",
    "# of training iterations, and set favourable batch sizes.\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, Layers, cost, num_iterations=100, alpha=.05):\n",
    "        self.num_iterations = num_iterations # iteration count\n",
    "        self.alpha = alpha # the learning rate\n",
    "            \n",
    "        if type(Layers) == list:\n",
    "            for layer in Layers:\n",
    "                if not isinstance(layer, Layer):\n",
    "                    raise EnvironmentError(\"All inputs should be of type 'Layer'\")\n",
    "        else:\n",
    "            raise EnvironmentError(\"Wrong input for Layers!\")\n",
    "        \n",
    "        for i in range(len(Layers)-1):\n",
    "            if Layers[i].shape[0] != Layers[i+1].shape[1]:\n",
    "                raise EnvironmentError(f\"The input size for Layer {i+2} should be \" + \\\n",
    "                                       f\"the same as the neurons count in Layer {i+1}!\")\n",
    "            \n",
    "        # softmax cross entropy (SCE for short) cost\n",
    "        if cost == \"SCE\":\n",
    "            self.cost = softmax_cross_entropy_cost\n",
    "            self.dcost = dsoftmax_cross_entropy_cost\n",
    "        else:\n",
    "            raise EnvironmentError(f\"There is no cost function such as {cost}\")\n",
    "                \n",
    "        self.Layers = Layers # The Layers in the neural net\n",
    "        self.X_train = None # data that the model will be trained with\n",
    "        self.y_hat, self.y_train = None, None # predictions, correct classes\n",
    "        self.J = [] # train losses\n",
    "        self.val_J = [] # validation losses\n",
    "        \n",
    "    def fit(self, X_train, y_train, X_val, y_val, batch_size=32):\n",
    "        if X_train.shape[0] != self.Layers[0].shape[1]:\n",
    "            raise EnvironmentError(f\"X_train should be of shape ({self.Layers[0].shape[0]}, N)\")\n",
    "        \n",
    "        if y_train.shape[0] != self.Layers[-1].shape[0]:\n",
    "            raise EnvironmentError(f\"y_train sholud be of shape({self.Layers[-1].shape[0]}, N)\")\n",
    "            \n",
    "        if X_val.shape[0] != self.Layers[0].shape[1]:\n",
    "            raise EnvironmentError(f\"X_val should be of shape ({self.Layers[0].shape[0]}, N)\")\n",
    "        \n",
    "        if y_val.shape[0] != self.Layers[-1].shape[0]:\n",
    "            raise EnvironmentError(f\"y_val sholud be of shape({self.Layers[-1].shape[0]}, N)\")\n",
    "            \n",
    "            \n",
    "        \n",
    "            \n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "        m = X_train.shape[1] # number of inputs\n",
    "        num_batches = m // batch_size\n",
    "        \n",
    "        report = \"Iteration {:3d}: training loss = {:.2f} | validation loss = {:.2f}\"\n",
    "        \n",
    "        for iteration in range(self.num_iterations):\n",
    "            train_loss = 0.\n",
    "        \n",
    "            for batch in range(num_batches):\n",
    "\n",
    "                # select a random mini-batch\n",
    "                idx = np.random.choice(m, batch_size, replace=False)\n",
    "                X_batch, y_batch = self.X_train[:, idx], self.y_train[:, idx]\n",
    "                \n",
    "                self.y_hat = self.predict(X_batch)\n",
    "                train_loss += self.cost(self.y_hat, y_batch)\n",
    "                self.update(X_batch, y_batch)\n",
    "\n",
    "            # report stats after each epoch\n",
    "            train_loss /= num_batches        \n",
    "            valid_loss = self.cost(self.predict(X_val), y_val)\n",
    "            \n",
    "            self.J.append(train_loss)\n",
    "            self.val_J.append(valid_loss)\n",
    "            print(report.format(iteration+1, train_loss, valid_loss))\n",
    "        \n",
    "    # update function: optimizing to achieve optimal values for wieghts and biases          \n",
    "    def update(self, X_batch, y_batch):\n",
    "\n",
    "        size = X_batch.shape[1]\n",
    "        derivatives = []\n",
    "        # derivative of loss with respect to the result of the linear part in the networks last layer (z^n)\n",
    "        dJ = self.dcost(self.y_hat, y_batch) \n",
    "        derivatives.append(dJ / size)\n",
    "\n",
    "        for i in range(len(self.Layers) - 2, -1, -1):\n",
    "            # derivative of loss with respect to the result of the linear part in the ith layer\n",
    "            # in order to have the right shape, we take the dot product of W^T with dl/dz^(i+1)\n",
    "            # and multiply it (elementwise) with the result of this layers' activation part\n",
    "            derivative = (self.Layers[i+1].W.T @ derivatives[-1]) * self.Layers[i].dActivation(self.Layers[i].z)\n",
    "            derivatives.append(derivative)\n",
    "            \n",
    "        derivatives.reverse()\n",
    "\n",
    "        # computing for every dl/dW^i and dl/db^i\n",
    "        for i in range(len(self.Layers) - 1, 0, -1):\n",
    "            self.Layers[i].W -= self.alpha *  derivatives[i] @ self.Layers[i-1].a.T\n",
    "            self.Layers[i].b -= self.alpha * np.sum(derivatives[i], axis=1, keepdims=True)\n",
    "        self.Layers[0].W -= self.alpha * derivatives[0] @ X_batch.T\n",
    "        self.Layers[0].b -= self.alpha * np.sum(derivatives[0], axis=1, keepdims=True)\n",
    "            \n",
    "    \n",
    "    def predict(self, X):\n",
    "        for i in range(len(self.Layers)):\n",
    "            self.Layers[i].train(X)\n",
    "            X = self.Layers[i].a\n",
    "        # result of feed forward\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shallow Network\n",
    "\n",
    "Let's create a model with only one Layer of 10 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork([\n",
    "\n",
    "    Layer(784, 10, Activation=\"soft\")\n",
    "                    ],\n",
    "    cost=\"SCE\",\n",
    "    num_iterations=20,\n",
    "    alpha=.005\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration   1: training loss = 1.54 | validation loss = 1.09\n",
      "Iteration   2: training loss = 0.94 | validation loss = 0.81\n",
      "Iteration   3: training loss = 0.75 | validation loss = 0.69\n",
      "Iteration   4: training loss = 0.66 | validation loss = 0.62\n",
      "Iteration   5: training loss = 0.60 | validation loss = 0.57\n",
      "Iteration   6: training loss = 0.56 | validation loss = 0.54\n",
      "Iteration   7: training loss = 0.53 | validation loss = 0.51\n",
      "Iteration   8: training loss = 0.52 | validation loss = 0.49\n",
      "Iteration   9: training loss = 0.50 | validation loss = 0.48\n",
      "Iteration  10: training loss = 0.48 | validation loss = 0.46\n",
      "Iteration  11: training loss = 0.46 | validation loss = 0.45\n",
      "Iteration  12: training loss = 0.45 | validation loss = 0.44\n",
      "Iteration  13: training loss = 0.44 | validation loss = 0.43\n",
      "Iteration  14: training loss = 0.44 | validation loss = 0.42\n",
      "Iteration  15: training loss = 0.43 | validation loss = 0.42\n",
      "Iteration  16: training loss = 0.42 | validation loss = 0.41\n",
      "Iteration  17: training loss = 0.41 | validation loss = 0.41\n",
      "Iteration  18: training loss = 0.41 | validation loss = 0.40\n",
      "Iteration  19: training loss = 0.40 | validation loss = 0.40\n",
      "Iteration  20: training loss = 0.40 | validation loss = 0.39\n"
     ]
    }
   ],
   "source": [
    "nn.fit(X_train, y_train, X_val, y_val, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy =   89.18%\n",
      "Validation accuracy = 89.05%\n",
      "Testing accuracy = 89.22%\n"
     ]
    }
   ],
   "source": [
    "train_acc = accuracy(nn.predict(X_train), y_train)\n",
    "valid_acc = accuracy(nn.predict(X_val), y_val)\n",
    "test_acc = accuracy(nn.predict(X_test), y_test)\n",
    "\n",
    "\n",
    "print('Training accuracy =   {:.2f}%'.format(train_acc))\n",
    "print('Validation accuracy = {:.2f}%'.format(valid_acc))\n",
    "print('Testing accuracy = {:.2f}%'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how do the weights look, based on what does the model classify? What does it see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x432 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(nn.Layers[0].W[i, :].reshape((28, 28)), cmap=plt.cm.coolwarm)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(\"%d\" % i)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks with 2 Layers\n",
    "\n",
    "Now let's create a model with one more additional layer of 20 neurons. Let's do the optimization steps for this one. Since there are more than one layer in the model, thus there are multiple sets of weights and biases, $W^{(1)}, W^{(n)}, \\cdots, W^{(n)}$ and $b^{(1)}, b^{(2)}, \\cdots,b^{(n)}$ for each respective layer, where $n$ denotes Layers count. The optimizaiton steps are shown below:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial W^{(n)}} = \\frac{\\partial\\mathscr{L}}{\\partial s} &\\cdot \\frac{\\partial s}{\\partial z^{(n)}} \\cdot \\frac{\\partial z^{(n)}}{\\partial W^{(n)}}\\\\\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial b^{(n)}} = \\frac{\\partial\\mathscr{L}}{\\partial s} &\\cdot \\frac{\\partial s}{\\partial z^{(n)}} \\cdot \\frac{\\partial z^{(n)}}{\\partial b^{(n)}}\n",
    "\\\\\\\\\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial W^{(n-1)}} = \\frac{\\partial\\mathscr{L}}{\\partial s} \\cdot \\frac{\\partial s}{\\partial z^{(n)}} &\\cdot \\frac{\\partial z^{(n)}}{\\partial a^{(n-1)}} \\cdot \\frac{\\partial a^{(n-1)}}{\\partial z^{(n-1)}} \\cdot \\frac{\\partial z^{(n-1)}}{\\partial W^{(n-1)}}\\\\\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial b^{(n-1)}} = \\frac{\\partial\\mathscr{L}}{\\partial s} \\cdot \\frac{\\partial s}{\\partial z^{(n)}} &\\cdot \\frac{\\partial z^{(n)}}{\\partial a^{(n-1)}} \\cdot \\frac{\\partial a^{(n-1)}}{\\partial z^{(n-1)}} \\cdot\\frac{\\partial z^{(n)}}{\\partial b^{(n-1)}}\n",
    "\\\\\\\\\n",
    "&\\vdots\n",
    "\\\\\\\\\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial W^{(1)}} = \\frac{\\partial\\mathscr{L}}{\\partial s} &\\cdot \\frac{\\partial s}{\\partial z^{(n)}} \\cdot \\cdots \\cdot \\frac{\\partial a^{(1)}}{\\partial z^{(1)}} \\cdot \\frac{\\partial z^{(1)}}{\\partial W^{(1)}}\\\\\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial b^{(1)}} = \\frac{\\partial\\mathscr{L}}{\\partial s} &\\cdot \\frac{\\partial s}{\\partial z^{(n)}} \\cdot \\cdots \\cdot \\frac{\\partial a^{(1)}}{\\partial z^{(1)}} \\cdot \\frac{\\partial z^{(1)}}{\\partial b^{(1)}}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite looking scary, if we pay a close attention, we can notice that:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial\\mathscr{L}}{\\partial W^{(i)}} = \\frac{\\partial\\mathscr{L}}{\\partial z^{(i+1)}} &\\cdot \\frac{\\partial z^{(i+1)}}{\\partial a^{(i)}} \\cdot \\frac{\\partial a^{(i)}}{\\partial z^{(i)}} \\cdot \\frac{\\partial z^{(i)}}{\\partial W^{(i)}}\\\\\n",
    "\\frac{\\partial\\mathscr{L}}{\\partial b^{(i)}} = \\frac{\\partial\\mathscr{L}}{\\partial z^{(i+1)}} &\\cdot \\frac{\\partial z^{(i+1)}}{\\partial a^{(i)}} \\cdot \\frac{\\partial a^{(i)}}{\\partial z^{(i)}} \\cdot \\frac{\\partial z^{(i)}}{\\partial b^{(i)}}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We can store $\\frac{\\partial\\mathscr{L}}{\\partial z^{(i)}}$ and use it for the next step. This will simplify the problem and reduce the computation time. The implementation of this can be seen in the `update` function of the `NeuralNetwork` class defined above:\n",
    "\n",
    "<img src='imgs/update.png' width='100%' height='100%' style=\"float:center\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "twoLayerNN = NeuralNetwork([\n",
    "    Layer(784, 20, Activation=\"ReLU\"),\n",
    "    Layer(20, 10, Activation=\"soft\")\n",
    "                    ],\n",
    "    cost=\"SCE\",\n",
    "    num_iterations=20,\n",
    "    alpha=.05\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration   1: training loss = 1.32 | validation loss = 0.55\n",
      "Iteration   2: training loss = 0.42 | validation loss = 0.36\n",
      "Iteration   3: training loss = 0.33 | validation loss = 0.30\n",
      "Iteration   4: training loss = 0.28 | validation loss = 0.27\n",
      "Iteration   5: training loss = 0.26 | validation loss = 0.26\n",
      "Iteration   6: training loss = 0.24 | validation loss = 0.24\n",
      "Iteration   7: training loss = 0.22 | validation loss = 0.23\n",
      "Iteration   8: training loss = 0.21 | validation loss = 0.22\n",
      "Iteration   9: training loss = 0.20 | validation loss = 0.21\n",
      "Iteration  10: training loss = 0.18 | validation loss = 0.21\n",
      "Iteration  11: training loss = 0.17 | validation loss = 0.20\n",
      "Iteration  12: training loss = 0.17 | validation loss = 0.19\n",
      "Iteration  13: training loss = 0.17 | validation loss = 0.19\n",
      "Iteration  14: training loss = 0.16 | validation loss = 0.18\n",
      "Iteration  15: training loss = 0.15 | validation loss = 0.18\n",
      "Iteration  16: training loss = 0.15 | validation loss = 0.18\n",
      "Iteration  17: training loss = 0.14 | validation loss = 0.18\n",
      "Iteration  18: training loss = 0.14 | validation loss = 0.17\n",
      "Iteration  19: training loss = 0.14 | validation loss = 0.17\n",
      "Iteration  20: training loss = 0.13 | validation loss = 0.17\n"
     ]
    }
   ],
   "source": [
    "twoLayerNN.fit(X_train, y_train, X_val, y_val, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy =   96.34%\n",
      "Validation accuracy = 94.78%\n",
      "Testing accuracy = 94.92%\n"
     ]
    }
   ],
   "source": [
    "train_acc = accuracy(twoLayerNN.predict(X_train), y_train)\n",
    "valid_acc = accuracy(twoLayerNN.predict(X_val), y_val)\n",
    "test_acc = accuracy(twoLayerNN.predict(X_test), y_test)\n",
    "\n",
    "\n",
    "print('Training accuracy =   {:.2f}%'.format(train_acc))\n",
    "print('Validation accuracy = {:.2f}%'.format(valid_acc))\n",
    "print('Testing accuracy = {:.2f}%'.format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
